{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUfd0q6BqUaXOSMeY0d7K9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nspeer12/AI_CAP4630/blob/master/hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yd7IjbAu-_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "26751445-8c21-4790-9097-4858b7c86f74"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "%tensorflow_version 2.x\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7AmdX8GvbAM",
        "colab_type": "text"
      },
      "source": [
        "Implement the function get_random_data(w, b, mu, sigma, m) that generates random data for logisitic regression with two features features x_1 and x_2. This function should return the array data of shape (m, 2) and the array labels of shape (m, 1).\n",
        "\n",
        "The entries of the arrays should be generated as follows. For each row i in {0, 1, ..., m-1}:\n",
        "\n",
        "Choose class label c=0 with probability 1/2 and c=1 with probability 1/2.\n",
        "Choose the first feature x_1 uniformly at random in the interval [0, 1).\n",
        "Set the second feature x_2 to be x_2 = w * x_1 + b + (-1)^c * n, where the \"noise\" n is chosen according to the normal distribution with mean mu and standard deviation sigma.\n",
        "The ith row of the array data consists of the features x_1 and x_2.\n",
        "The ith entry of the vector labels is the class label c.\n",
        "Implement the function display_random_data that takes as input the above two arrays labels and data. It should create scatter plot of the 2D points stored in data. Use red dots to plot the points whose labels are 1 and blue dots to plot the points whose labels are 0.\n",
        "\n",
        "Hints: You should see that the 2D points (feature vectors) corresponding to different classes are approximately separated by the line y = w * x + b, where w and b are the parameters that you used to generate the data. Note that the smaller the parameter mu, the closer the points are to this line. Also, the larger the parameter sigma, the more points can be on the wrong side of this line.\n",
        "\n",
        "Experiment with different values of mu and sigma. Make sure that the parameter m is large enough so you have enough data points.\n",
        "\n",
        "Split the data/labels into a training set (80%) and a test set (20%).\n",
        "\n",
        "Links to the numpy documentation of the functions that can be used to draw samples according to the uniform and normal distributions:\n",
        "\n",
        "Normal distribution\n",
        "Uniform distribution\n",
        "You can learn more about the normal distribution on https://en.wikipedia.org/wiki/Normal_distribution. To gain some intuition, it would be helpful to plot the Gaussian function for different parameters mu and sigma in a seperate notebook (that you do not have to submit). Later in the semester, you will need to work with normal distribution to understand variational autoencoders.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxmywQ7WvRCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(999)\n",
        "\n",
        "def get_random_data(w, b, mu, sigma, m):\n",
        "  c = [0 if np.random.rand(1) < .5 else 1 for x in range(m)]\n",
        "  alt_c = [(-1)**i for i in c]\n",
        "\n",
        "  x_1 = np.random.uniform(low=0, high=1, size=m)\n",
        "  x_2 = np.add(w * x_1, np.add(b, np.dot(alt_c, np.random.normal(loc=mu, scale=sigma, size=m))))\n",
        "\n",
        "  colors = [None] * m\n",
        "  for x in range(len(colors)):\n",
        "    colors[x] = ('blue' if c[x] == 1 else 'red')\n",
        "\n",
        "  return x_1, x_2, c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajKZIqjLvsba",
        "colab_type": "code",
        "outputId": "efd857f4-e4bd-4ed2-df5f-80a0cc724111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "def graph_random_data():\n",
        "  x_1, x_2, c = get_random_data(5,1,5,100,100)\n",
        "  fig, ax = plt.subplots(figsize=(5,5))\n",
        "\n",
        "  cmap = ['red' if k == 1 else 'blue' for k in c]\n",
        "  ax.scatter(x_1, x_2, c=cmap)\n",
        "  plt.show()\n",
        "\n",
        "graph_random_data()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEvCAYAAAAzcMYwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGMo8dKDwbfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcoboTTZCcb6",
        "colab_type": "text"
      },
      "source": [
        "**Problem 2**\n",
        "\n",
        "Create a Keras to implement logistic regression with two features and train it with the data generated in Problem 1. The loss should be the binary cross entropy loss. \n",
        "\n",
        "How well does the trained model separate the red and blue dots?  You can obtain the separating line determined by the model by extracting the weights from the dense layer using the function ```get_weights```. See [https://keras.io/layers/about-keras-layers/](https://keras.io/layers/about-keras-layers/).  \n",
        "\n",
        "Create a plot showing the random data, the true line used to generate the data, and the separating line of the trained model. Make sure that you describe in detail in your notebook how you proceed to obtain the separating line.\n",
        "\n",
        "Note that you have to carry out some simple steps to obtain the separating line from the model weights (the two weights and the bias term of the dense layer).  This is not immediately obvious.  It maybe helpful to take a look at the heatmap below.\n",
        "\n",
        "The trained model realizes function ```f : R^2 -> R``` that takes two features as input and outputs a number in the interval ```[0, 1]```. Use a heatmap to visualize this function.  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqmnBwlzCdd2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14244728-bf82-48a7-99a9-d3b8c601442e"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.regularizers import L1L2\n",
        "\n",
        "x_1, x_2, y = get_random_data(5,5,5,5,100)\n",
        "X = np.array([x_1, x_2]).T\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='softmax', input_dim=X.shape[1]))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 75 samples, validate on 25 samples\n",
            "Epoch 1/100\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 2/100\n",
            "75/75 [==============================] - 0s 96us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 3/100\n",
            "75/75 [==============================] - 0s 117us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 4/100\n",
            "75/75 [==============================] - 0s 105us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 5/100\n",
            "75/75 [==============================] - 0s 231us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 6/100\n",
            "75/75 [==============================] - 0s 113us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 7/100\n",
            "75/75 [==============================] - 0s 128us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 8/100\n",
            "75/75 [==============================] - 0s 105us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 9/100\n",
            "75/75 [==============================] - 0s 166us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 10/100\n",
            "75/75 [==============================] - 0s 119us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 11/100\n",
            "75/75 [==============================] - 0s 148us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 12/100\n",
            "75/75 [==============================] - 0s 101us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 13/100\n",
            "75/75 [==============================] - 0s 133us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 14/100\n",
            "75/75 [==============================] - 0s 177us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 15/100\n",
            "75/75 [==============================] - 0s 133us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 16/100\n",
            "75/75 [==============================] - 0s 126us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 17/100\n",
            "75/75 [==============================] - 0s 81us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 18/100\n",
            "75/75 [==============================] - 0s 86us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 19/100\n",
            "75/75 [==============================] - 0s 99us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 20/100\n",
            "75/75 [==============================] - 0s 110us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 21/100\n",
            "75/75 [==============================] - 0s 91us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 22/100\n",
            "75/75 [==============================] - 0s 97us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 23/100\n",
            "75/75 [==============================] - 0s 82us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 24/100\n",
            "75/75 [==============================] - 0s 90us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 25/100\n",
            "75/75 [==============================] - 0s 95us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 26/100\n",
            "75/75 [==============================] - 0s 144us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 27/100\n",
            "75/75 [==============================] - 0s 123us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 28/100\n",
            "75/75 [==============================] - 0s 110us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 29/100\n",
            "75/75 [==============================] - 0s 103us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 30/100\n",
            "75/75 [==============================] - 0s 105us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 31/100\n",
            "75/75 [==============================] - 0s 118us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 32/100\n",
            "75/75 [==============================] - 0s 115us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 33/100\n",
            "75/75 [==============================] - 0s 96us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 34/100\n",
            "75/75 [==============================] - 0s 107us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 35/100\n",
            "75/75 [==============================] - 0s 99us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 36/100\n",
            "75/75 [==============================] - 0s 117us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 37/100\n",
            "75/75 [==============================] - 0s 107us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 38/100\n",
            "75/75 [==============================] - 0s 245us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 39/100\n",
            "75/75 [==============================] - 0s 117us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 40/100\n",
            "75/75 [==============================] - 0s 274us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 41/100\n",
            "75/75 [==============================] - 0s 115us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 42/100\n",
            "75/75 [==============================] - 0s 106us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 43/100\n",
            "75/75 [==============================] - 0s 113us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 44/100\n",
            "75/75 [==============================] - 0s 145us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 45/100\n",
            "75/75 [==============================] - 0s 141us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 46/100\n",
            "75/75 [==============================] - 0s 110us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 47/100\n",
            "75/75 [==============================] - 0s 98us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 48/100\n",
            "75/75 [==============================] - 0s 130us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 49/100\n",
            "75/75 [==============================] - 0s 111us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 50/100\n",
            "75/75 [==============================] - 0s 104us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 51/100\n",
            "75/75 [==============================] - 0s 108us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 52/100\n",
            "75/75 [==============================] - 0s 111us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 53/100\n",
            "75/75 [==============================] - 0s 89us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 54/100\n",
            "75/75 [==============================] - 0s 104us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 55/100\n",
            "75/75 [==============================] - 0s 108us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 56/100\n",
            "75/75 [==============================] - 0s 85us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 57/100\n",
            "75/75 [==============================] - 0s 81us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 58/100\n",
            "75/75 [==============================] - 0s 95us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 59/100\n",
            "75/75 [==============================] - 0s 85us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 60/100\n",
            "75/75 [==============================] - 0s 88us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 61/100\n",
            "75/75 [==============================] - 0s 96us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 62/100\n",
            "75/75 [==============================] - 0s 167us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 63/100\n",
            "75/75 [==============================] - 0s 111us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 64/100\n",
            "75/75 [==============================] - 0s 116us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 65/100\n",
            "75/75 [==============================] - 0s 134us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 66/100\n",
            "75/75 [==============================] - 0s 112us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 67/100\n",
            "75/75 [==============================] - 0s 103us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 68/100\n",
            "75/75 [==============================] - 0s 123us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 69/100\n",
            "75/75 [==============================] - 0s 115us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 70/100\n",
            "75/75 [==============================] - 0s 116us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 71/100\n",
            "75/75 [==============================] - 0s 116us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 72/100\n",
            "75/75 [==============================] - 0s 113us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 73/100\n",
            "75/75 [==============================] - 0s 245us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 74/100\n",
            "75/75 [==============================] - 0s 133us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 75/100\n",
            "75/75 [==============================] - 0s 137us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 76/100\n",
            "75/75 [==============================] - 0s 179us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 77/100\n",
            "75/75 [==============================] - 0s 176us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 78/100\n",
            "75/75 [==============================] - 0s 174us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 79/100\n",
            "75/75 [==============================] - 0s 134us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 80/100\n",
            "75/75 [==============================] - 0s 163us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 81/100\n",
            "75/75 [==============================] - 0s 155us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 82/100\n",
            "75/75 [==============================] - 0s 134us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 83/100\n",
            "75/75 [==============================] - 0s 134us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 84/100\n",
            "75/75 [==============================] - 0s 120us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 85/100\n",
            "75/75 [==============================] - 0s 141us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 86/100\n",
            "75/75 [==============================] - 0s 140us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 87/100\n",
            "75/75 [==============================] - 0s 148us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 88/100\n",
            "75/75 [==============================] - 0s 132us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 89/100\n",
            "75/75 [==============================] - 0s 146us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 90/100\n",
            "75/75 [==============================] - 0s 134us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 91/100\n",
            "75/75 [==============================] - 0s 122us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 92/100\n",
            "75/75 [==============================] - 0s 154us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 93/100\n",
            "75/75 [==============================] - 0s 126us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 94/100\n",
            "75/75 [==============================] - 0s 127us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 95/100\n",
            "75/75 [==============================] - 0s 123us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 96/100\n",
            "75/75 [==============================] - 0s 112us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 97/100\n",
            "75/75 [==============================] - 0s 113us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 98/100\n",
            "75/75 [==============================] - 0s 95us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 99/100\n",
            "75/75 [==============================] - 0s 115us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n",
            "Epoch 100/100\n",
            "75/75 [==============================] - 0s 107us/step - loss: 6.8021 - acc: 0.5733 - val_loss: 7.6523 - val_acc: 0.5200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8be8d4d438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a7uegBsDIiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}